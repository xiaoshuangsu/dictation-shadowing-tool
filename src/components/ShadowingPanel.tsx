"use client"

import { useState, useEffect, useRef } from "react"

interface Sentence {
  id: number
  text: string
  startTime: number
  endTime: number
}

interface ShadowingPanelProps {
  sentence: Sentence
  onComplete?: (isCorrect: boolean) => void
  onNext?: () => void
  isLastSentence?: boolean
}

export default function ShadowingPanel({ sentence, onComplete, onNext, isLastSentence }: ShadowingPanelProps) {
  const [isRecording, setIsRecording] = useState(false)
  const [recognition, setRecognition] = useState<any>(null)
  const [userTranscript, setUserTranscript] = useState("")
  const [showResult, setShowResult] = useState(false)
  const [micError, setMicError] = useState<string | null>(null)

  // å½•éŸ³ç›¸å…³
  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(null)
  const [recordedAudioUrl, setRecordedAudioUrl] = useState<string | null>(null)
  const audioRef = useRef<HTMLAudioElement>(null)
  const originalAudioRef = useRef<HTMLAudioElement>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const recognitionRef = useRef<any>(null)
  const sentenceRef = useRef(sentence)
  const onCompleteRef = useRef(onComplete)
  const userTranscriptRef = useRef(userTranscript)
  const recordedChunksRef = useRef<Blob[]>([])
  const recordedMimeTypeRef = useRef<string>('')

  // æ›´æ–° refs å½“å€¼å˜åŒ–æ—¶
  useEffect(() => {
    sentenceRef.current = sentence
  }, [sentence])

  useEffect(() => {
    onCompleteRef.current = onComplete
  }, [onComplete])

  useEffect(() => {
    userTranscriptRef.current = userTranscript
  }, [userTranscript])

  useEffect(() => {
    setUserTranscript("")
    setShowResult(false)
    setRecordedAudioUrl(null)
    setMicError(null)
    recordedChunksRef.current = []
  }, [sentence.id])

  // åˆå§‹åŒ– MediaRecorder å’Œ SpeechRecognition
  useEffect(() => {
    if (typeof window === 'undefined') return

    // æ£€æµ‹æ”¯æŒçš„éŸ³é¢‘ MIME ç±»å‹
    const getSupportedMimeType = () => {
      const types = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/ogg;codecs=opus',
        'audio/ogg',
        'audio/mp4',
        'audio/mp3',
        ''
      ]
      for (const type of types) {
        if (MediaRecorder.isTypeSupported(type)) {
          return type
        }
      }
      return ''
    }

    // åˆå§‹åŒ– MediaRecorder
    if (navigator.mediaDevices) {
      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
          const supportedType = getSupportedMimeType()
          console.log("Supported MIME type:", supportedType || 'default')
          recordedMimeTypeRef.current = supportedType

          // iOS Safari æ”¯æŒ
          const options = supportedType ? { mimeType: supportedType } : undefined
          const recorder = new MediaRecorder(stream, options)
          setMediaRecorder(recorder)
          mediaRecorderRef.current = recorder

          recorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
              recordedChunksRef.current.push(event.data)
              console.log("Chunk received, total chunks:", recordedChunksRef.current.length)
            }
          }

          recorder.onstop = () => {
            console.log("Recorder stopped, chunks:", recordedChunksRef.current.length)
            if (recordedChunksRef.current.length > 0) {
              const blob = new Blob(recordedChunksRef.current, { type: recordedMimeTypeRef.current || 'audio/webm' })
              const url = URL.createObjectURL(blob)
              console.log("Created audio URL:", url)
              setRecordedAudioUrl(url)
              recordedChunksRef.current = []
            } else {
              console.error("No audio data recorded")
            }
          }

          // MediaRecorder å‡†å¤‡å¥½åï¼Œåˆå§‹åŒ– SpeechRecognition
          if ('webkitSpeechRecognition' in window) {
            const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition
            const recog = new SpeechRecognition()
            recog.continuous = false
            recog.interimResults = true
            recog.lang = 'en-US'

            recog.onresult = (event: any) => {
              let interimTranscript = ''
              for (let i = event.resultIndex; i < event.results.length; i++) {
                const result = event.results[i]
                if (result.isFinal) {
                  interimTranscript = result[0].transcript
                  console.log("Final transcript:", interimTranscript)
                  setUserTranscript(interimTranscript)
                  setShowResult(true)

                  // ä½¿ç”¨ sentenceRef.current è·å–æœ€æ–°çš„ sentence
                  const currentSentence = sentenceRef.current
                  // ç›´æ¥åœ¨è¿™é‡Œåˆ¤æ–­æ­£ç¡®æ€§
                  const normalize = (text: string) => text.toLowerCase().replace(/[^\w\s]/g, "").replace(/\s+/g, " ").trim()
                  const isCorrect = normalize(interimTranscript) === normalize(currentSentence.text)
                  console.log("Pronunciation correct:", isCorrect, "Expected:", currentSentence.text)

                  // å»¶è¿Ÿè°ƒç”¨ onCompleteï¼Œç¡®ä¿çŠ¶æ€æ›´æ–°åå†è§¦å‘ transcript æ›´æ–°
                  setTimeout(() => {
                    if (onCompleteRef.current) {
                      onCompleteRef.current(isCorrect)
                    }
                  }, 100)
                }
              }
            }

            recog.onerror = (event: any) => {
              console.error("Speech recognition error:", event.error)
              setMicError(`è¯­éŸ³è¯†åˆ«é”™è¯¯: ${event.error}`)
              setIsRecording(false)
            }

            recog.onend = () => {
              // è¯­éŸ³è¯†åˆ«ç»“æŸæ—¶ï¼ŒåŒæ—¶åœæ­¢éŸ³é¢‘å½•åˆ¶
              if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
                mediaRecorderRef.current.stop()
              }
              setIsRecording(false)
            }

            setRecognition(recog)
            recognitionRef.current = recog
          } else {
            setMicError("æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½")
          }
        })
        .catch(err => {
          console.error("Error accessing microphone:", err)
          // æ£€æµ‹æ˜¯å¦æ˜¯ç§»åŠ¨è®¾å¤‡
          const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent)
          if (isMobile) {
            setMicError("ç§»åŠ¨ç«¯å½•éŸ³åŠŸèƒ½å—é™ï¼Œå»ºè®®ä½¿ç”¨ç”µè„‘æµè§ˆå™¨è¿›è¡Œå½±å­è·Ÿè¯»ç»ƒä¹ ")
          } else {
            setMicError("æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æµè§ˆå™¨æƒé™è®¾ç½®")
          }
        })
    } else {
      setMicError("æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒå½•éŸ³åŠŸèƒ½ï¼Œå»ºè®®ä½¿ç”¨æœ€æ–°ç‰ˆ Chrome æˆ– Edge æµè§ˆå™¨")
    }

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop()
      }
    }
  }, [])

  // å¼€å§‹å½•éŸ³ï¼ˆè¯­éŸ³è¯†åˆ« + éŸ³é¢‘å½•åˆ¶ï¼‰
  const startRecording = () => {
    setMicError(null)
    if (!recognitionRef.current || !mediaRecorderRef.current) {
      setMicError("å½•éŸ³åŠŸèƒ½æœªåˆå§‹åŒ–å®Œæˆï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•")
      return
    }

    try {
      setUserTranscript("")
      setShowResult(false)
      setRecordedAudioUrl(null)
      recordedChunksRef.current = []

      // å¼€å§‹è¯­éŸ³è¯†åˆ«
      recognitionRef.current.start()
      // å¼€å§‹éŸ³é¢‘å½•åˆ¶
      mediaRecorderRef.current.start()

      setIsRecording(true)
    } catch (err) {
      console.error("Error starting recording:", err)
      setMicError("å¯åŠ¨å½•éŸ³å¤±è´¥ï¼Œè¯·é‡è¯•")
      setIsRecording(false)
    }
  }

  // åœæ­¢å½•éŸ³
  const stopRecording = () => {
    if (recognitionRef.current && mediaRecorderRef.current && isRecording) {
      try {
        recognitionRef.current.stop()
        mediaRecorderRef.current.stop()
        setIsRecording(false)
      } catch (err) {
        console.error("Error stopping recording:", err)
      }
    }
  }

  // æ’­æ”¾ç”¨æˆ·å½•éŸ³
  const playRecording = async () => {
    if (audioRef.current && recordedAudioUrl) {
      try {
        audioRef.current.currentTime = 0
        await audioRef.current.play()
      } catch (err) {
        console.error("Error playing recording:", err)
      }
    }
  }

  // æ’­æ”¾åŸéŸ³
  const playOriginal = () => {
    if (originalAudioRef.current) {
      const audio = originalAudioRef.current
      audio.currentTime = sentence.startTime
      audio.play()

      // åœ¨å¥å­çš„ç»“æŸæ—¶é—´åœæ­¢æ’­æ”¾
      const durationToPlay = (sentence.endTime - sentence.startTime) * 1000
      setTimeout(() => {
        audio.pause()
      }, durationToPlay + 100)
    }
  }

  // åˆ¤æ–­è¯»éŸ³æ­£ç¡®æ€§
  const normalizeText = (text: string) => {
    return text.toLowerCase().replace(/[^\w\s]/g, "").replace(/\s+/g, " ").trim()
  }

  const isCorrect = userTranscript ? normalizeText(userTranscript) === normalizeText(sentence.text) : false

  return (
    <div>
      {/* åŸéŸ³æ’­æ”¾å™¨ï¼ˆéšè—ï¼‰ */}
      <audio
        ref={originalAudioRef}
        src="/dictation-shadowing-tool/learn-english-via-listening-1001.mp3"
      />

      <p className="text-sm text-gray-500 mb-4">
        ğŸ’¡ å½±å­è·Ÿè¯»ï¼šæ’­æ”¾éŸ³é¢‘åï¼Œç‚¹å‡»éº¦å…‹é£è·Ÿè¯»
      </p>

      {/* å‚è€ƒæ–‡æœ¬ */}
      <div className="bg-gray-50 rounded-lg p-4 mb-6">
        <p className="text-sm text-gray-500 mb-1">åŸå¥ï¼š</p>
        <p className="text-base text-gray-800">{sentence.text}</p>
      </div>

      {/* é”™è¯¯æç¤º */}
      {micError && (
        <div className="mb-4 p-3 rounded-lg bg-red-50 border border-red-200">
          <p className="text-sm text-red-700">{micError}</p>
        </div>
      )}

      {/* ç”¨æˆ·è¯»éŸ³ç»“æœ */}
      {showResult && userTranscript && (
        <div className={`mb-4 p-4 rounded-lg border-2 ${
          isCorrect
            ? "bg-green-50 border-green-300"
            : "bg-orange-50 border-orange-300"
        }`}>
          <p className="text-xs text-gray-500 mb-1">ä½ çš„å‘éŸ³ï¼š</p>
          <p className="text-base text-gray-800 mb-3">{userTranscript}</p>

          {/* æ­£ç¡®æ€§åˆ¤æ–­ */}
          {isCorrect ? (
            <div className="flex items-center gap-2 text-green-600">
              <svg className="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                <path fillRule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm3.707-9.293a1 1 0 00-1.414-1.414L9 10.586 7.707 9.293a1 1 0 00-1.414 1.414l2 2a1 1 0 001.414 0l4-4z" clipRule="evenodd" />
              </svg>
              <span className="font-medium">å‘éŸ³å‡†ç¡®ï¼</span>
            </div>
          ) : (
            <div className="flex items-center gap-2 text-orange-600">
              <svg className="w-5 h-5" fill="currentColor" viewBox="0 0 20 20">
                <path fillRule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clipRule="evenodd" />
              </svg>
              <span className="font-medium">ç»§ç»­åŠ æ²¹ï¼</span>
            </div>
          )}
        </div>
      )}

      {/* éŸ³é¢‘å¯¹æ¯”åŒºåŸŸ */}
      {recordedAudioUrl && (
        <div className="mb-4 p-4 bg-blue-50 rounded-lg border border-blue-200">
          <p className="text-sm font-medium text-gray-700 mb-3">ğŸµ éŸ³é¢‘å¯¹æ¯”</p>

          <div className="flex gap-3">
            {/* æ’­æ”¾åŸéŸ³ */}
            <button
              onClick={playOriginal}
              className="flex-1 flex items-center justify-center gap-2 py-2 px-4 bg-white border border-gray-300 rounded-lg hover:bg-gray-50 transition-colors"
            >
              <svg className="w-5 h-5 text-blue-600" fill="currentColor" viewBox="0 0 24 24">
                <path d="M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z"/>
              </svg>
              <span className="text-sm font-medium text-gray-700">åŸéŸ³</span>
            </button>

            {/* æ’­æ”¾æˆ‘çš„å½•éŸ³ */}
            <button
              onClick={playRecording}
              className="flex-1 flex items-center justify-center gap-2 py-2 px-4 bg-white border border-gray-300 rounded-lg hover:bg-gray-50 transition-colors"
            >
              <svg className="w-5 h-5 text-green-600" fill="currentColor" viewBox="0 0 24 24">
                <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3s-3 1.34-3 3v6c0 1.66 1.34 3 3 3z"/>
                <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
              </svg>
              <span className="text-sm font-medium text-gray-700">æˆ‘çš„å½•éŸ³</span>
            </button>
          </div>

          {/* éŸ³é¢‘æ’­æ”¾å™¨ */}
          <audio
            ref={audioRef}
            src={recordedAudioUrl}
            controls
            className="w-full mt-3"
            onError={(e) => console.error("Audio error:", e)}
          />
        </div>
      )}

      {/* å¤§éº¦å…‹é£æŒ‰é’® */}
      <div className="flex justify-center mb-6">
        <button
          onClick={isRecording ? stopRecording : startRecording}
          disabled={!recognition || !mediaRecorder}
          className={`w-24 h-24 rounded-full flex items-center justify-center transition-all ${
            isRecording
              ? "bg-red-500 text-white scale-110 shadow-lg"
              : "bg-blue-500 text-white hover:bg-blue-600 hover:scale-105 shadow-md disabled:bg-gray-300 disabled:cursor-not-allowed disabled:hover:scale-100"
          }`}
        >
          <svg className={`w-8 h-8 ${isRecording ? "animate-pulse" : ""}`} fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3s-3 1.34-3 3v6c0 1.66 1.34 3 3 3z"/>
            <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
          </svg>
        </button>
      </div>

      {/* Next æŒ‰é’® */}
      <div className="flex justify-center mb-6">
        <button
          onClick={() => {
            if (onNext && !isLastSentence) {
              onNext()
            }
          }}
          disabled={isLastSentence}
          className="px-6 py-2 bg-green-500 text-white rounded-lg font-medium hover:bg-green-600 disabled:bg-gray-300 disabled:cursor-not-allowed transition-colors flex items-center gap-2"
        >
          Next
          <svg className="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M9 5l7 7-7 7" />
          </svg>
        </button>
      </div>

      {/* å½•éŸ³çŠ¶æ€æç¤º */}
      {isRecording && (
        <div className="text-center mb-4">
          <p className="text-sm text-red-500 animate-pulse">ğŸ¤ æ­£åœ¨å½•éŸ³...</p>
        </div>
      )}
    </div>
  )
}
